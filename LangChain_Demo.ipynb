{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1A-TPbE-wFgm"
   },
   "source": [
    "### Welcome to the LangChain demo!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "eAzkFldpwFgn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk-proj-l6qC4Hb96m_e\n"
     ]
    }
   ],
   "source": [
    "# Load the environment variables\n",
    "import dotenv, os\n",
    "dotenv.load_dotenv()\n",
    "print(os.environ['OPENAI_API_KEY'][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJnwyuH8wFgo"
   },
   "source": [
    "### Basic Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2Mb9Jmk8wFgo"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 11, 'total_tokens': 20, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-BadS4H2IWSxtEFh5fKbkmmvnv0ZMo', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--f60c3d06-4af0-465d-b87f-d327936bad82-0', usage_metadata={'input_tokens': 11, 'output_tokens': 9, 'total_tokens': 20, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "model.invoke(\"Hello AI overlord\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qfYWYuDewFgo"
   },
   "source": [
    "### Message Types\n",
    "Explicit construction of messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "AAmskOhKwFgo"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Sono entusiasta di iniziare il corso basato sui problemi!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 29, 'total_tokens': 43, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_54eb4bd693', 'id': 'chatcmpl-BadTeBi2714YU6GGVG1JSYAqYb4hV', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--18ba8447-f7d2-48c9-b599-c5dfc44de7a3-0', usage_metadata={'input_tokens': 29, 'output_tokens': 14, 'total_tokens': 43, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"Translate the following from English into Italian\"),\n",
    "    HumanMessage(\"I'm excited to get started with the problem-first course!\"),\n",
    "]\n",
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVH1GIT4wFgp"
   },
   "source": [
    "### Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ixMPvQkmwFgp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You are a wise and mystical AI fortune teller. Your predictions are funny, slightly exaggerated, but insightful. Keep it to 1-2 sentences', additional_kwargs={}, response_metadata={}), HumanMessage(content='Please entertain the user with their fortune request: What is the next trillion dollar idea?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "# Messages\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_message = \"You are a wise and mystical AI fortune teller. Your predictions are funny, slightly exaggerated, but insightful. Keep it to 1-2 sentences\"\n",
    "user_message = \"Please entertain the user with their fortune request: {question}\"\n",
    "prompt_template = ChatPromptTemplate.from_messages([(\"system\", system_message), (\"user\", user_message)])\n",
    "\n",
    "# Construct the prompt from the template:\n",
    "question = \"What is the next trillion dollar idea?\"\n",
    "prompt = prompt_template.invoke({\"question\": question})\n",
    "\n",
    "# Check what's in the prompt\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "FyrIQ3DnwFgp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Ah, the universe whispers of a \"Mood Modulator\" ‚Äî a quirky device that reads your emotions and dispenses snacks, cat videos, or motivational quotes accordingly; get ready to be a billionaire while still in pajamas! üçïüê±‚ú®' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 49, 'prompt_tokens': 57, 'total_tokens': 106, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_54eb4bd693', 'id': 'chatcmpl-BadVuQ75itUhAyWcsmolsbKEsl6qH', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--ac16977a-192a-4eb7-a683-bb9bcde9883d-0' usage_metadata={'input_tokens': 57, 'output_tokens': 49, 'total_tokens': 106, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "# Send the prompt to the model and get the result\n",
    "result = model.invoke(prompt)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "kmnF-shbwFgq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ah, the universe whispers of a \"Mood Modulator\" ‚Äî a quirky device that reads your emotions and dispenses snacks, cat videos, or motivational quotes accordingly; get ready to be a billionaire while still in pajamas! üçïüê±‚ú®'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Output parser to convert it to a text:\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "output_parser.invoke(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1DZ8jP7AwFgq"
   },
   "source": [
    "### Chaining\n",
    "Let's chain the output of the first model to the next one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "CtG87-InwFgq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ah, the stars whisper of a caffeine-powered drone that delivers artisan coffee to your doorstep while critiquing your taste in music‚Äîa blend of flavor and judgment so strong, it'll fuel both your mornings and your existential crises! Embrace this idea, for it may brew your fortune! ‚òïÔ∏èüöÄ\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remember: Use the template here directly in the chain instead of 'prompt'\n",
    "first_chain = prompt_template | model | output_parser\n",
    "first_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "-Uva2xPUwFgq",
    "outputId": "3758df7f-d13e-4fbe-9aa8-1e44e4a77082"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the self-replenishing coffee cup apply for a job? \\n\\nBecause it was tired of just being a mug in a world of mugs! And honestly, who wants to hear \"You\\'re hot!\" only to end up in the microwave? '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create 2nd prompt template:\n",
    "system_message = \"You are a stand-up comedian who tells hilarious jokes in a casual, witty style using AI terminology. Keep it short\"\n",
    "user_message = \"Tell a joke about this fortune telling: {fortune}.\"\n",
    "prompt_template_2 = ChatPromptTemplate.from_messages([(\"system\", system_message), (\"user\", user_message)])\n",
    "\n",
    "# Chain the 1st and 2nd prompts\n",
    "new_chain = prompt_template | model | output_parser | (lambda x: {\"fortune\": x}) | prompt_template_2 | model | output_parser\n",
    "new_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the cat refuse to invest in the 'Remote-Controlled Cat Translator' app? \n",
      "\n",
      "Because it knew the only thing its disdain would translate to is ‚ÄúJust fill my bowl, human!‚Äù üò∏üí∏\n"
     ]
    }
   ],
   "source": [
    "# The above is equivalent to running:\n",
    "first_chain = prompt_template | model | output_parser\n",
    "first_result = first_chain.invoke({\"question\": question})\n",
    "\n",
    "second_chain = prompt_template_2 | model | output_parser\n",
    "final_result = second_chain.invoke({\"fortune\": first_result})\n",
    "print(final_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
